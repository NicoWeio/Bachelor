\section{CORN}
% Conditional Ordinal Regression for Neural Networks
\emph{CORN} (\textbf{C}onditional \textbf{O}rdinal \textbf{R}egression for \textbf{N}eural Networks) \cite{corn}
is a framework for ordinal classification in neural networks.
It is based on the ideas of binary subtasks and conditional probabilities
and improves upon its predecessor \emph{CORAL} \cite{coral}.
\todo{
  Go on…
  - Niu et al.'s approach has rank inconsistency (also quoted below)
  - CORAL's weight sharing limited the expressiveness of the network
}

% ORIG: Instead, CORN uses a new training procedure with conditional training subsets that ensures rank consistency through applying the chain rule of probability.


\subsection{Method} \label{sec:corn:method}
\todo{This subsection heavily borrows from the CORN paper. Is that okay?}

Let $D = \left\{ \mathbf{x}^{[i]}, y^{[i]} \right\}_{i=1}^N$ denote a dataset of $N$ training examples,
where $\mathbf{x}^{[i]}$ is the $i$-th example and $y^{[i]}$ is its class label.
Since the class labels are ordinal, they are referred to as \emph{rank} labels.
Each rank label is an element of the set of all ranks $\{r_1, r_2, \ldots, r_K\}$,
  where
  $K$ is the number of ranks
  and $r_1 < r_2 < \ldots < r_K$.

The rank labels $y^{[i]}$ are used to create a set of binary subtasks,
so that for every rank label $y^{[i]}$,
$K$ subtasks are created. % TODO: Not K-1 ?
  % which are denoted by $y^{[i]}_k$.
Each subtask $y^{[i]}_k \in \{0, 1\}$ is a binary classification task,
  where
    $y^{[i]}_k = 1$ if $y^{[i]} > r_k$ (in words: {$y^{[i]}$ exceeds rank $r_k$})
    and $y^{[i]}_k = 0$ otherwise.
This method of creating binary subtasks is referred to as \emph{extended binary classification} \cite{extended_binary}.
% …and predates CORN.

Given a test example $\mathbf{x}^{[i]}$
and probability predictions $f_k(\mathbf{x}^{[i]}) \in [0,1]$ for each subtask $k$,
the \emph{rank index} $q \in \{1, 2, \ldots, K\}$ is computed as
\begin{equation}
  q = \sum_{k=1}^{K-1} \mathbb{1}\left\{f_k(\mathbf{x}^{[i]}) > 0.5\right\} \ .
\end{equation}
The predicted rank label is then obtained via
$h(\mathbf{x}^{[i]}) = r_q$,
where $h: \mathcal{X} \to \mathcal{Y}$ is the mapping from input space $\mathcal{X}$ to output space $\mathcal{Y}$ %,
which minimizes the CORN loss function.


\emph{Rank monotony} describes a desirable property of the rank labels,
whereby the probability of exceeding a rank $r_k$ is always greater than or equal to the probability of exceeding a higher rank $r_{k+1}$.
% a rank label being exceeded by a higher rank label is always higher than the probability of being exceeded by a lower rank label.
%
While not strictly necessary for the computation of rank indices $q$ (or ranks $r_q$),
it is intuitively clear that imposing such a restriction could improve the quality of predictions.

\textsc{Corn} ensures rank monotony
  $f_1(\mathbf{x}^{[i]}) \leq f_2(\mathbf{x}^{[i]}) \leq \ldots \leq f_{K-1}(\mathbf{x}^{[i]})$
by applying the chain rule of probability
to the conditional probabilities
\begin{equation}
  f_k(\mathbf{x}^{[i]}) = \hat{P}\left( y^{[i]} > r_k \mid y^{[i]} > r_{k-1} \right) \ .
\end{equation}



% TODO: Add a TikZ visualization similar to Fig. 1 in the CORN paper | see content/tikz/rank_consistency.tex

\todo{
  Not integrated with the text yet.
  Put more emphasis on the conditional training subsets.
  Maybe move to appendix.
}
The loss function is provided by CORN.
Its definition is shown in \autoref{eqn:corn:loss}.
\begin{equation}
  \label{eqn:corn:loss}
  L(\mathbf{X}, \mathbf{y}) =
  - \frac{1}{\sum_{j=1}^{K-1} |S_j|}
  \sum_{j=1}^{K-1}
  \sum_{i=1}^{|S_j|}
  \left[
    \log(f_j(\mathbf{x}^{[i]})) · \mathbb{1}\left\{y^{[i]} > r_j\right\}
    +
    \log(1 - f_j(\mathbf{x}^{[i]})) · \mathbb{1}\left\{y^{[i]} \leq r_j\right\}
  \right]
\end{equation}


\subsection{Obtaining probabilities from CORN} % OR: Converting threshold to per-class probabilities
As explained before,
% CORN returns conditional (?) probabilities based on the binary classification subtasks it uses.
\textsc{Corn} only returns a single predicted rank $r_q$ for a given input $\mathbf{x}$.
% … and internally has no notion of “absolute” per-class probabilities.
% NOTE: Don't get confused here:
% - the output layer represents the conditional probabilities of exceeding a rank label
% - the chain rule of probability is used to ensure rank monotony and converts the conditional probabilities into probabilities of exceeding a rank label
% - the rank index $q$ is computed from the probabilities of exceeding a rank label
In contrast,
the DSEA algorithm is strongly coupled to the idea of per-class probabilities (confidences).
Therefore, a conversion is necessary.
Under the assumption that all energies belong to one of the ranks (bins),
such a conversion is possible.

As an example, given four rank indices $q \in \{0, 1, 2, 3\}$,
the conditional probabilities are
\begin{align*}
  % NOTE: P[q>(-1)] = 1 and P[q>3] = 0 :)
  P[q=0] &= 1 - P[q>0] \\
  P[q=1] &= P[q>1] - P[q>1] \\
  P[q=2] &= P[q>1] - P[q>2] \\
  P[q=3] &= P[q>2]
\end{align*}

A more detailed explanation is given in \autoref{sec:appendix:corn_probas}.


\subsection{…}
% TODO: This does not belong to the subsection "Getting probabilities from CORN"
In order to support the weighting of individual samples,
the CORN loss function is modified
to include the weight as a factor.
The updated code is shown in \autoref{sec:appendix:corn_weighting}.
