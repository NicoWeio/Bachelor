\section{CORN} \label{sec:ordinal:corn}
% Conditional Ordinal Regression for Neural Networks
% NOTE: Ordinal Regression = Ordinal Classification. It's an intermediate problem between classification and regression. (→ corn)
\acf{CORN} \cite{corn}
is a framework for ordinal classification in neural networks.
It is based on the ideas of binary subtasks and conditional probabilities
and improves upon
  its direct predecessor \ac{CORAL} \cite{coral}
  as well as the approach by \citeauthor{extended_binary_nn} \cite{extended_binary_nn}.
%
% NOTE: - Niu et al.'s approach has rank inconsistency
%       - CORAL has rank consistency,
%           but is limited in expressiveness due to its weight sharing.
%
% ORIG: Instead, CORN uses a new training procedure with conditional training subsets
% that ensures rank consistency through applying the chain rule of probability.
%
% COULDDO: It might make sense to explain the flaws of the previous approaches here,
% but that would also require explaining rank consistency,


\subsection{Method} \label{sec:corn:method}
% COULDDO: This subsection heavily borrows from the CORN paper.
% Although it's cited, it could be rephrased some more.
% @Leonora thinks it's fine as is.
%
Let $D = \left\{ \mathbf{x}^{[i]}, y^{[i]} \right\}_{i=1}^N$ denote a data set of $N$ training examples,
where $\mathbf{x}^{[i]}$ is the $i$-th example and $y^{[i]}$ is its class label.
Since the class labels are ordinal, they are referred to as \emph{rank} labels.
Each rank label is an element of the set of all ranks $\{r_1, r_2, \ldots, r_K\}$,
  where
  $K$ is the number of ranks
  and $r_1 < r_2 < \ldots < r_K$.

For every rank label $y^{[i]}$,
$K - 1$ subtasks are created.
Each subtask $y^{[i]}_k \in \{0, 1\}$ is a binary classification task,
  where
    $y^{[i]}_k = 1$ if $y^{[i]} > r_k$ (in words: {$y^{[i]}$ exceeds rank $r_k$})
    and $y^{[i]}_k = 0$ otherwise.
This method of creating binary subtasks is referred to as \emph{extended binary classification} \cite{extended_binary}.
% …and predates CORN.

Given a test example $\mathbf{x}^{[i]}$
and probability predictions $f_k(\mathbf{x}^{[i]}) \in [0,1]$ for each subtask $k$,
the \emph{rank index} $q \in \{1, 2, \ldots, K\}$ is computed as
\begin{equation}
  q = \sum_{k=1}^{K-1} \mathbb{1}\left\{f_k(\mathbf{x}^{[i]}) > 0.5\right\} \ .
\end{equation}
The predicted rank label is then obtained via
$h(\mathbf{x}^{[i]}) = r_q$,
where $h: \mathcal{X} \to \mathcal{Y}$ is the mapping from input space $\mathcal{X}$ to output space $\mathcal{Y}$ %,
which minimizes the CORN loss function.


\emph{Rank monotony} describes a desirable property of the rank labels,
whereby the probability of exceeding a rank $r_k$ is always greater than or equal to the probability of exceeding a higher rank $r_{k+1}$.
% a rank label being exceeded by a higher rank label is always higher than the probability of being exceeded by a lower rank label.
%
While not strictly necessary for the computation of rank indices $q$ (or ranks $r_q$),
it is intuitively clear that imposing such a restriction could improve the quality of predictions.
%
\Ac{CORN} ensures rank monotony
  $f_1(\mathbf{x}^{[i]}) \leq f_2(\mathbf{x}^{[i]}) \leq \ldots \leq f_{K-1}(\mathbf{x}^{[i]})$
by applying the chain rule of probability
\begin{equation}
  \hat{P}\left(y^{[i]} > r_k\right) = \prod_{j=1}^k f_j(\mathbf{x}^{[i]})
\end{equation}
to the conditional probabilities
% COULDDO: It should be noted at the first mention (above) that these are conditional probabilities.
\begin{equation}
  f_k(\mathbf{x}^{[i]}) = \hat{P}\left( y^{[i]} > r_k \mid y^{[i]} > r_{k-1} \right) \ .
  \label{eq:corn:conditional_probabilities}
\end{equation}
%
% COULDDO: Add a TikZ visualization similar to Fig. 1 in the CORN paper | see content/tikz/rank_consistency.tex

\Ac{CORN} also provides the loss function.
The conditional nature of the predictions $f_k(\mathbf{x}^{[i]})$
  (see \autoref{eq:corn:conditional_probabilities})
is respected
by splitting the training data into conditional training subsets $S_k$
  each time the loss function is computed:
\begin{align*}
  S_1 &: \text{all } \left\{\left( \mathbf{x}^{[i]}, y^{[i]} \right)\right\} \, \text{ for } i \in \{1, \ldots, N\} \, , \\
  S_2 &: \left\{\left( \mathbf{x}^{[i]}, y^{[i]} \right) \mid y^{[i]} > r_1 \right\} \, , \\
  &\cdots \\
  % NOTE: I made `K-2` uppercase, because I think it's lowercase by mistake in the paper.
  S_{K-1} &: \left\{\left( \mathbf{x}^{[i]}, y^{[i]} \right) \mid y^{[i]} > r_{K-2} \right\} \, .
\end{align*}
In words,
for $k > 1$,
  $S_k$ contains all training examples
  for which the rank label exceeds rank $r_{k-1}$
    (or, equivalently, for which the observed probability of exceeding rank $r_{k-1}$ is $1$).
The first subset $S_1$ contains all training examples.

The definition of the loss function is shown in \autoref{eqn:corn:loss}.
Here, $|S_k|$ denotes the number of elements in the subset $S_k$.
\begin{multline}
  \label{eqn:corn:loss}
  L(\mathbf{X}, \mathbf{y}) =
  - \frac{1}{\sum_{j=1}^{K-1} |S_j|}
  \sum_{j=1}^{K-1}
  \sum_{i=1}^{|S_j|}
  \Bigl[
    \log(f_j(\mathbf{x}^{[i]})) · \mathbb{1}\left\{y^{[i]} > r_j\right\}
    \\
    +
    \log(1 - f_j(\mathbf{x}^{[i]})) · \mathbb{1}\left\{y^{[i]} \leq r_j\right\}
  \Bigr]
\end{multline}


\subsection{Obtaining Probabilities from CORN} \label{sec:ordinal:corn:probas}
% OR: Converting threshold to per-class probabilities
As explained in \autoref{sec:corn:method},
% CORN returns conditional (?) probabilities based on the binary classification subtasks it uses.
\Ac{CORN} only returns a single predicted rank $r_q$ for a given input $\mathbf{x}$.
% … and internally has no notion of “absolute” per-class probabilities.
% NOTE: Don't get confused here:
% - the output layer represents the conditional probabilities of exceeding a rank label
% - the chain rule of probability is used to ensure rank monotony and converts the conditional probabilities into probabilities of exceeding a rank label
% - the rank index $q$ is computed from the probabilities of exceeding a rank label
In contrast,
\dsea{} is strongly coupled to the idea of per-class probabilities (confidences).
Therefore, a conversion is necessary.
Under the assumption that all energies belong to one of the ranks (bins),
such a conversion is realizable.

As an example, given four rank indices $q \in \{0, 1, 2, 3\}$,
the conditional probabilities are
\newcommand{\myP}{\hat{P}}
\begin{align*}
  % NOTE: P[q>(-1)] = 1 and P[q>3] = 0 :)
  \myP(q=0) &= 1 - \myP(q>0) \\
  \myP(q=1) &= \myP(q>1) - \myP(q>1) \\
  \myP(q=2) &= \myP(q>1) - \myP(q>2) \\
  \myP(q=3) &= \myP(q>2)
\end{align*}

% COULDDO: Explain in layman's terms (@Leonora)
A more detailed explanation is given in \autoref{sec:appendix:corn_probas}.


% \subsection{…}
% COULDDO: This does not belong to the subsection "Getting probabilities from CORN", but isn't really a subsection on its own either.
% In order to support the weighting of individual samples,
% the CORN loss function is modified
% to include the weight as a factor.
% The updated code is shown in \autoref{sec:appendix:corn_weighting}.
% NOTE: I followed @Karolins suggestion to remove the code completely.
