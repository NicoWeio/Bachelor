\chapter{Summary and Outlook} \label{sec:summary}
% should be 1 page

% █ What did I achieve?
It has been shown that
the combination of neural networks, ordinality and \dsea{}
  can be successfully applied to
  the problem of neutrino energy spectrum estimation
  with \icecube{} data.
This was enabled by
  adding support for
    sample weights
    and confidences
  to \ac{CORN}.

% █ comparisons
The new method is not unambiguously superior to the previous ones
  (\cite{dsea_jan} and \cite{dsea_samuel}).
A strict comparison is not possible
    in the first place,
  as this work introduces under-/overflow bins
  and makes use of adaptive step sizes.
% 1. confidence distributions → Samuel
The randomly selected confidence distributions of a common neural network using softmax \cite{dsea_samuel}
  are of comparable quality to those obtained in this work (see \autoref{fig:bootstrap:single_events}),
    even though ordinality is disregarded in the former case.
  % Compared to the confidence distributions of \emph{LogisticAT} \cite{dsea_jan}, …
%
% 2. accuracy → Samuel
Compared to \cite{dsea_samuel},
  higher \hyperref[sec:unfolding:metrics:accuracy]{accuracy} is achieved
    (\SI{42.7}{\percent} vs. $< \SI{39}{\percent}$).
Both the \ac{RMSE}
  (\num{0.0164} vs. \num{0.000269})
and the $\chi^2$ distance
  (\num{0.0392} vs. \num{0.003123})
are worse,
  however,
because of the large deviations in higher energy bins.
%
% 3. Wasserstein distance → Jan
In comparison to \cite{dsea_jan},
  similar \hyperref[sec:unfolding:metrics:wd]{Wasserstein distances} are achieved
    (ours: \num{0.0108} vs. theirs: \num{0.00879}),
    but using \num{10} instead of \num{12} bins.
On the other hand,
our probability distributions of single events are not strictly unimodal.


% █ future work
There is still a multitude of ways in which \dsea{} and the application thereof could be improved.
%
Explicit \hyperref[sec:dsea:deconvolution_problem:regularization]{regularization}
could dampen the currently observed oscillations in higher energy bins.
%
Other hyperparameters,
  such as the shape of the neural network,
are yet to be optimized.
%
It might be possible to modify \ac{CORN}
  so that the per-class confidence distributions are strictly unimodal.
In general,
other neural network architectures
  could be investigated.
For example,
  graph neural networks
  already exceed boosted decision trees
    in terms of both resolution and speed \cite{minh2021gnn}.
%
Graph neural networks
have the additional benefit of
being less dependent on feature engineering
  as they can be applied to \enquote{raw} data
    (%
      which \ac{DOM} was hit,
      the collected charge, % Oxford comma
      and time of arrival%
    ).
%
Finally,
  more data could be used for training.
Because of the shape of the spectrum,
  the number of events in the highest energy bins
  remains relatively small
    relative to the complete dataset.
Although this thesis has \hyperref[sec:unfolding:bias]{demonstrated that \dsea{} eliminates the training bias},
  the effect of stratified training data on the overall performance
  has not been investigated.
% NOTE: We can't stratify the real-world data, only the training data.


% COULDDO:
% \section{(Comparison to Conventional Neural Networks)}
% \section{(Comparison to LogisticAT)}
