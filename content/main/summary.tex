\chapter{Summary and Outlook} \label{sec:summary}
% should be 1 page

% █ What did I achieve?
It has been shown that
the combination of neural networks, ordinality and \dsea{}
  can be successfully applied to
  the problem of neutrino energy spectrum estimation
  with \icecube{} data.
This was enabled by
  adding support for
    sample weights
    and confidences
  to \ac{CORN}.

% █ comparisons
In contrast to previous works \cite{dsea_jan, dsea_samuel},
  adaptive step sizes \cite{dsea_mirko} are used in \dsea{},
    % This isn't exactly verified in this thesis…
    drastically reducing the number of iterations required for convergence.
Furthermore,
  under- and overflow bins are added
    to allow for the application to real data.

% With regard to the resulting spectra,
The new method is not unambiguously superior to the previous ones:
% 1. confidence distributions → Samuel
  The randomly selected confidence distributions of a common neural network using softmax \cite{dsea_samuel}
    are of comparable quality to those obtained in this work (see \autoref{fig:bootstrap:single_events}),
      even though ordinality is disregarded in the former case.
  % Compared to the confidence distributions of \emph{LogisticAT} \cite{dsea_jan}, …
%
% 2. accuracy → Samuel
The accuracy …
% 3. Wasserstein distance → Jan
The Wasserstein distance …
% The resulting spectra are not unambiguously better

A strict comparison is not possible,
  as this work introduces under-/overflow bins.

Compared to \cite{dsea_jan},
  better \hyperref[sec:unfolding:metrics:wd]{Wasserstein distances} are achieved
    (\num{0.00879} vs. TODO),
    but using \num{10} instead of \num{12} bins.
On the other hand,
the probability distributions of single events are not strictly unimodal.


% █ future work
There is still a multitude of ways in which \dsea{} and the application thereof could be improved.
%
Explicit \hyperref[sec:dsea:deconvolution_problem:regularization]{regularization}
could dampen the currently observed oscillations in higher-energy bins.
%
Other hyperparameters,
  such as the shape of the neural network,
are yet to be optimized.
%
It might be possible to modify \ac{CORN}
  so that the per-class confidence distributions are strictly unimodal.
In general,
other neural network architectures
  could be investigated.
For example,
  graph neural networks
  already exceed boosted decision trees
    in terms of both resolution and speed \cite{minh2021gnn}.
%
Graph neural networks
have the additional benefit of
being less dependent on feature engineering
  as they can be applied to \enquote{raw} data
    (%
      which \ac{DOM} was hit,
      the collected charge, % Oxford comma
      and time of arrival%
    ).
%
Finally,
  more data could be used for training.
Because of the shape of the spectrum,
  the number of events in the highest energy bins
  remains relatively small
    relative to the complete dataset.
Although this thesis has \hyperref[sec:unfolding:bias]{demonstrated that \dsea{} eliminates the training bias},
  the effect of stratified training data on the overall performance
  has not been investigated.
% NOTE: We can't stratify the real-world data, only the training data.


% COULDDO:
% \section{(Comparison to Conventional Neural Networks)}
% \section{(Comparison to LogisticAT)}
