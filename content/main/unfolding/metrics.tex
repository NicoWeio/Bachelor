\section{Performance metrics}
In order to evaluate the performance of the models
and to compare them to prior works,
several metrics are used.
%
For the calculation of all metrics that are mentioned here are,
scikit-learn \cite{sklearn} is used.


\subsection{Accuracy} \label{sec:unfolding:metrics:accuracy}
The \emph{accuracy} \cite{accuracy} is the fraction of correctly classified events to the total number of events.
It is a common metric for classification tasks,
but it is not ideal for ordinal classification
  since it does not take the ordering of the classes into account.
For example,
the metric is the same for
a misclassification by one rank
and a misclassification by two ranks.
%
Nonetheless,
it gives an indication of the overall performance of the model.


\subsection{Mean absolute error} \label{sec:unfolding:metrics:mae}
The \emph{mean absolute error} (\textsc{Mae}) \cite{mae} is a metric that is commonly used for regression tasks. % TODO
It is defined as
\begin{equation}
  \text{\textsc{Mae}} = \frac{1}{N} \sum_{i=1}^N \left| y^{[i]} - \hat{y}^{[i]} \right|
\end{equation}
where $N$ is the number of events,
$y^{[i]}$ is the true value of the $i$-th event,
and $\hat{y}^{[i]}$ is the predicted value of the $i$-th event. % Oxford comma

% Because of its simple definition,
% it can be easily understood and interpreted.

Since the absolute value of the error is considered,
overestimation and underestimation are treated equally
and do not cancel each other out.
% NOTE: The referenced paper gives some good arguments for using MAE over RMSE.
In contrast to the \emph{root mean squared error} (\textsc{Rmse}),
the \textsc{Mae} is not especially sensitive to outliers
and has a more natural interpretation \cite{mae}.


\subsection{Wasserstein distance} \label{sec:unfolding:metrics:wd}
The two previous metrics were based on single predictions for each event.
They disregard both
  the confidences of the predictions,
    considering only the prediction with the highest confidence,
  and the spectrum,
    which results from summing of the confidences over all events.

In contrast,
% NOTE: The cited paper calls it "Earth Mover's Distance", but I want the citation here for consistency.
the \emph{Wasserstein distance} \cite{wd}
compares the unfolded spectrum to the true spectrum.
%
It is also known as \emph{earth mover's distance} (\textsc{Emd}),
  hinting at the analogy of moving earth to transform one distribution into another,
    where the cost is given as the product of the distance and the amount of earth moved.
      % essentially the physical work.

Mathematically, the Wasserstein distance (of first kind) can be defined as
\begin{equation}
  \text{\textsc{Wd}}(\mathbf{p}, \mathbf{q}) = \inf_{\pi \in \Pi(\mathbf{p}, \mathbf{q})} \int_{\mathbb{R}^2} |x - y| \, \mathrm{d}\pi(x, y)
\end{equation}
where
  $\mathbf{p}$ and $\mathbf{q}$ are the probability distributions subject to comparison,
  $\Pi(\mathbf{p}, \mathbf{q})$ is the set of all probability distributions on $\mathbb{R}^2$,
  and $\pi$ is a probability distribution on $\mathbb{R}^2$.
% The Wasserstein distance is zero if and only if $\mathbf{p} = \mathbf{q}$.
