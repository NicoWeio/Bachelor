\section{Setup}

\subsection{Dataset}
The unmodified dataset consists of about 13 million Monte-Carlo simulated \enquote{upgoing} neutrino events,
% NOTE: Exact number is 13336413 before any preprocessing.
with energies ranging from \SI{100}{\giga\electronvolt} to almost \SI{E8}{\giga\electronvolt}.
% TODO: Explain “upgoing”.
See \autoref{fig:dataset:raw:histogram} for a histogram of the data.
% TODO: Number of features: 79 (Jan) or 99-1 (my notebook)?

To ensure comparability to the works of \citeauthor{dsea_samuel} as well as \citeauthor{dsea_jan},
\num{500000} events are used for training.
% This also allows for more thorough hyperparameter optimization,
% which would otherwise be limited by the computational resources available as well as the timeframe of this thesis.


\subsection{Feature selection}
The \emph{mRMR} (Minimum Redundancy Maximum Relevance) feature selection algorithm \cite{mrmr} is employed to select the 12 most relevant features.
This has already been done in \cite{dsea_jan}.
A list of said features is provided in \autoref{tab:features_best}.

\begin{table}
    \centering
    \caption{
      Best features according to the \emph{mRMR} algorithm \cite{dsea_jan},
      ordered by …. % TODO
    }
    \label{tab:features_best}
    \begin{tabular}{l}
        \toprule
        % \midrule
        % NOTE: MCPrimary.energy is excluded
        \texttt{SplineMPEDirectHitsICE.n\_dir\_doms} \\
        \texttt{VariousVariables.Cone\_Angle} \\
        \texttt{SplineMPECramerRaoParams.variance\_theta} \\
        \texttt{Borderness.Q\_ratio\_in\_border} \\
        \texttt{SplineMPETruncatedEnergy\_SPICEMie\_BINS\_MuEres.value} \\
        \texttt{SplineMPETruncatedEnergy\_SPICEMie\_DOMS\_Neutrino.energy} \\
        \texttt{SplineMPEDirectHitsICB.n\_late\_doms} \\
        \texttt{Dustyness.n\_doms\_in\_dust} \\
        \texttt{LineFitGeoSplit1Params.n\_hits} \\
        \texttt{SplineMPEDirectHitsICC.dir\_track\_hit\_distribution\_smoothness} \\
        \texttt{SPEFit2GeoSplit1BayesianFitParams.logl} \\
        \texttt{SplineMPECharacteristicsIC.avg\_dom\_dist\_q\_tot\_dom} \\
        \bottomrule
    \end{tabular}
\end{table}


\subsection{Transformation}
It has been shown that none of the selected features are normally distributed \cite{dsea_jan}.
In accordance with \cite{dsea_jan},
the features are therefore transformed using the \emph{Yeo-Johnson} transformation \cite{yeo_johnson},
a power transformation which reduces skewness.
% https://en.wikipedia.org/wiki/Power_transform#Yeo–Johnson_transformation
% TODO: Verify positive effect on performance
% TODO: Add formula

Furthermore, all features are scaled by their standard deviation.


\subsection{Discretization}
% As described in …, DSEA requires the energies to be discretized.
The target variable \texttt{MCPrimary.energy} is discretized into \num{10} logarithmically spaced bins
(in accordance with \cite{dsea_samuel}).
%
In order to allow for the application to real data,
under- and overflow bins are added.
%
The overflow bin is chosen so that it contains a similar number of events as the previous bin,
ensuring sufficient statistics.
A lower energy limit of \SI{E5}{\giga\electronvolt} (in accordance with \cite{dsea_samuel}) was found to satisfy this requirement.
%
The underflow bin is assigned the energy range \SI{E2}{\giga\electronvolt} to $10^{2.1} \si{\giga\electronvolt}$,
% \qtyrange{E2}{E2.1}{\giga\electronvolt}.
because the dataset does not contain any events with very low energies below \SI{E2}{\giga\electronvolt}.
Again, the event count in the underflow bin is chosen to be similar to the neighboring bin.

A histogram utilizing the aforementioned bins is shown in \autoref{fig:dataset:discretized:histogram}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{example-image-a}
  \caption{Energy spectrum of the full, untouched Monte-Carlo dataset using the discretized energy ranges as bins.}
  \label{fig:dataset:discretized:histogram}
\end{figure}


\subsection{Neural network}
\citeauthor{corn} provide a PyTorch \cite{pytorch} implementation of their CORN algorithm
as well as several examples demonstrating its application on different datasets.
% TODO: Reference their tabular data / cement example?

This work makes use of said implementation of CORN
and hence the PyTorch framework.
Additionally, PyTorch Lightning \cite{pytorch_lightning} and TorchMetrics \cite{torch_metrics} are used.

The neural network consists of \num{4} fully connected hidden layers.
The input layer has \num{12} neurons,
  corresponding to the number of features,
while the output layer has \num{9} neurons,
  corresponding to the number of binary classification subtasks,
    i.e. the number of bins minus one.
% CORN in the output layer…
% In total, the neural network has \num{TODO} neurons.

The number of neurons in the hidden layers is shown in \autoref{tab:nn_shape}.
\begin{itemize}
  \item Leaky ReLU activation function
  \item Fully connected layers
  \item Adam optimizer
\end{itemize}

\begin{table}
  \centering
  \caption{
    Shape and activation functions of the neural network.
    The number of neurons in the input and output layers is determined by the number of features and bins, respectively.
  }
  \label{tab:nn_shape}
  \begin{tabular}{S[table-format=3.0] c}
    \toprule
    {neurons} & {activation function} \\
    \midrule
    12  & – \\
    120 & leaky ReLU \\
    240 & leaky ReLU \\
    120 & leaky ReLU \\
    12  & leaky ReLU \\
    9   & leaky ReLU \\
    \bottomrule
  \end{tabular}
\end{table}

The loss function is provided by CORN.
Its definition is shown in \autoref{eqn:corn:loss}.
\begin{equation}
  \label{eqn:corn:loss}
  L(\mathbf{X}, \mathbf{y}) =
  - \frac{1}{\sum_{j=1}^{K-1} |S_j|}
  \sum_{j=1}^{K-1}
  \sum_{i=1}^{|S_j|}
  \left[
    \log(f_j(\mathbf{x}^{[i]})) · \mathbb{1}\left\{y^{[i]} > r_j\right\}
    +
    \log(1 - f_j(\mathbf{x}^{[i]})) · \mathbb{1}\left\{y^{[i]} \leq r_j\right\}
  \right]
\end{equation}

\emph{ADAM} (Adaptive Moment Estimation) \cite{adam} is used as the optimizer.
% It combines the benefits of both AdaGrad and RMSProp.

The neural network keeps its weights between DSEA iterations.
As demonstrated in \cite{dsea_samuel}, % (\texttt{one\_model})
this has no significant effect on the performance.


\subsection{DSEA}
For this work, the Python implementation of DSEA \cite{dsea_code} by \citeauthor{dsea_mirko} is used.
It expects a \emph{scikit-learn} \cite{sklearn} classifier,
In order to interface with the reference implementation of DSEA \cite{dsea_code},
a wrapper class is implemented,
  which exposes a constructor as well as the needed methods
  \mintinline{python}{fit(X, y, sample_weight)} and
  \mintinline{python}{predict_proba(X)}.
