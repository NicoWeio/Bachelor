\section{Setup}

\subsection{Dataset}
The unmodified dataset consists of about 13 million Monte-Carlo simulated \enquote{upgoing} neutrino events,
% NOTE: Exact number is 13336413 before any preprocessing.
with energies ranging from \SI{100}{\giga\electronvolt} to almost \SI{E8}{\giga\electronvolt}.
% TODO: Explain “upgoing”.
See \autoref{fig:dataset:raw:histogram} for a histogram of the data.
% TODO: Number of features: 79 (Jan) or 99-1 (my notebook)?

To ensure comparability to \cite{dsea_samuel},
\num{500000} events are used for training.


\subsection{Feature selection}
The \emph{mRMR} (Minimum Redundancy Maximum Relevance) feature selection algorithm \cite{mrmr} is employed to select the 12 most relevant features.
This has already been done in \cite{dsea_jan}.
A list of said features is provided in \autoref{tab:features_best}.

\begin{table}
    \centering
    \caption{
      Best features according to the \emph{mRMR} algorithm \cite{dsea_jan},
      ordered by …. % TODO
    }
    \label{tab:features_best}
    \begin{tabular}{l}
        \toprule
        % \midrule
        % NOTE: MCPrimary.energy is excluded
        \texttt{SplineMPEDirectHitsICE.n\_dir\_doms} \\
        \texttt{VariousVariables.Cone\_Angle} \\
        \texttt{SplineMPECramerRaoParams.variance\_theta} \\
        \texttt{Borderness.Q\_ratio\_in\_border} \\
        \texttt{SplineMPETruncatedEnergy\_SPICEMie\_BINS\_MuEres.value} \\
        \texttt{SplineMPETruncatedEnergy\_SPICEMie\_DOMS\_Neutrino.energy} \\
        \texttt{SplineMPEDirectHitsICB.n\_late\_doms} \\
        \texttt{Dustyness.n\_doms\_in\_dust} \\
        \texttt{LineFitGeoSplit1Params.n\_hits} \\
        \texttt{SplineMPEDirectHitsICC.dir\_track\_hit\_distribution\_smoothness} \\
        \texttt{SPEFit2GeoSplit1BayesianFitParams.logl} \\
        \texttt{SplineMPECharacteristicsIC.avg\_dom\_dist\_q\_tot\_dom} \\
        \bottomrule
    \end{tabular}
\end{table}


\subsection{Transformation}
It has been shown that none of the selected features are normally distributed \cite{dsea_jan}.
In accordance with \cite{dsea_jan},
the features are transformed using the \emph{Yeo-Johnson} transformation \cite{yeo_johnson},
a power transformation which reduces skewness.
% https://en.wikipedia.org/wiki/Power_transform#Yeo–Johnson_transformation
% TODO: Verify positive effect on performance
% TODO: Add formula

Furthermore, all features are scaled by their standard deviation.


% TODO: This will change once Under-/Overflow bins are implemented
% \subsection{Energy cut}


\subsection{Discretization}
% As described in …, DSEA requires the energies to be discretized.
The target variable \texttt{MCPrimary.energy} is discretized into \num{10} logarithmically spaced bins
(in accordance with \cite{dsea_samuel}).
%
In order to allow for the application to real data,
under- and overflow bins are added.
%
The overflow bin is chosen so that it contains a similar number of events as the previous bin,
ensuring sufficient statistics.
An lower energy limit of \SI{E5}{\giga\electronvolt} (in accordance with \cite{dsea_samuel}) was found to satisfy this requirement.
%
The underflow bin is assigned the energy range \SI{E2}{\giga\electronvolt} to $10^{2.1} \si{\giga\electronvolt}$,
% \qtyrange{E2}{E2.1}{\giga\electronvolt}.
because the dataset does not contain any events with very low energies below \SI{E2}{\giga\electronvolt}.
Again, the event count in the underflow bin is chosen to be similar to the neighboring bin.

A histogram utilizing the aforementioned bins is shown in \autoref{fig:dataset:discretized:histogram}.


\subsection{Neural network}
% The neural network consists of \num{3} fully connected layers.
The first layer has \num{12} neurons,
  corresponding to the number of features,
while the last layer has \num{9} neurons,
  corresponding to the number of binary classification subtasks,
    i.e. the number of bins minus one.

The number of neurons in the hidden layers is shown in \autoref{tab:nn_shape}.
\begin{itemize}
  \item Leaky ReLU activation function
  \item Fully connected layers
  \item Adam optimizer
\end{itemize}

PyTorch \cite{pytorch} is used to implement the neural network.
Additionally, PyTorch Lightning \cite{pytorch_lightning} and TorchMetrics \cite{torch_metrics} are used.

\begin{table}
  \centering
  \caption{
    Shape and activation functions of the neural network.
    The number of neurons in the input and output layers is determined by the number of features and bins, respectively.
  }
  \label{tab:nn_shape}
  \begin{tabular}{S[table-format=3.0] c}
    \toprule
    {neurons} & {activation function} \\
    \midrule
    12  & – \\
    120 & leaky ReLU \\
    240 & leaky ReLU \\
    120 & leaky ReLU \\
    12  & leaky ReLU \\
    9   & leaky ReLU \\
    \bottomrule
  \end{tabular}
\end{table}

The loss function is provided by CORN.
Its definition is shown in \autoref{eqn:corn:loss}.
\begin{equation}
  \label{eqn:corn:loss}
  L(\mathbf{X}, \mathbf{y}) =
  - \frac{1}{\sum_{j=1}^{K-1} |S_j|}
  \sum_{j=1}^{K-1}
  \sum_{i=1}^{|S_j|}
  \left[
    \log(f_j(\mathbf{x}^{[i]})) · \mathbb{1}\left\{y^{[i]} > r_j\right\}
    +
    \log(1 - f_j(\mathbf{x}^{[i]})) · \mathbb{1}\left\{y^{[i]} \leq r_j\right\}
  \right]
\end{equation}

\emph{ADAM} (Adaptive Moment Estimation) \cite{adam} is used as the optimizer.
% It combines the benefits of both AdaGrad and RMSProp.

In order to interface with the reference implementation of DSEA \cite{dsea_code},
  which expects a \emph{scikit-learn} \cite{sklearn} classifier,
a wrapper class is implemented,
  which exposes a constructor as well as the needed methods
  \mintinline{python}{fit(X, y, sample_weight)} and
  \mintinline{python}{predict_proba(X)}.

The neural network keeps its weights between DSEA iterations.
As demonstrated in \cite{dsea_samuel}, % (\texttt{one\_model})
this has no significant effect on the performance.
