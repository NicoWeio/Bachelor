\clearpage % guidance only
\section{Hyperparameters}
Hyperparameters are parameters that are not learned from the data,
but adjusted by the user.
%
In order to find the optimal hyperparameters,
the unfolding procedure is repeated for different values of the hyperparameters.

% The hyperparameters which are expected to be mostly independent of others are checked first (see \autoref{sec:hyperparameters:initial_bayesian}).
% Then, the remaining hyperparameters are checked in separate grid searches.

A starting point is determined by Bayesian optimization of the hyperparameters
  (see \autoref{sec:hyperparameters:initial_bayesian}).
Then, individual grid searches are performed for the hyperparameters of interest.
%
In each case, 10-fold cross-validation is utilized to reduce the variance of single data points
  (see \autoref{sec:hyperparameters:cross_validation}).

% ↓ see pp. 46-48 of dsea_mirko
The adaptive step size method of \dseaplus{} is used
because it performs well with all reasonable convergence thresholds.
It eliminates the need to optimize for step size related hyperparameters,
  such as
    exponential vs. multiplicative decay
    or the initial step size,
  apart from the $J$-factor,
    which only has a minor effect on the results,
and provides accurate results after a few iterations \cite{dsea_mirko}.

Since \textsc{Adam} maintains separate learning rates for each parameter,
optimization for the learning rate is not essential.
This is verified by the initial Bayesian optimization (\autoref{sec:hyperparameters:initial_bayesian}),
  where the results have no significant correlation with the learning rate.


\subsection{Cross-validation} \label{sec:hyperparameters:cross_validation}
% TODO: citation?
Cross-validation is a method to evaluate the performance of a model on unseen data
without having to reserve a part of the data for testing.

The data is split into $k$ \emph{folds} of equal size. % not to be confused with DSEA's iteration index k
The model is then trained on $k-1$ folds,
and evaluated on the remaining fold.
This is repeated $k$ times,
each time using a different fold for evaluation.
The average of the results is then used as the performance metric.

In this work,
cross-validation is used primarily to get more meaningful performance metrics
for each hyperparameter setting,
  % which would otherwise [be noisy / good by chance / …].
  since individual runs have a high variance.
The number of folds is set to $k = 10$,
  striking a balance between
    the size of the dataset,
    the statistical uncertainty,
    and the computational cost.


% “These are the baseline hyperparameters as determined by a Bayesian optimization search.
% Grid searches for individual hyperparameters will show that these are actually optimal.”
% (I hope.)
\subsection{Initial Bayesian search} \label{sec:hyperparameters:initial_bayesian}
Since a grid search is computationally expensive,
  especially for a large number of hyperparameters,
a Bayesian optimization search \cite{wandb_bayesian} is used to find a good starting point for the grid search.
% TODO: Explain a little…

\autoref{fig:hyperparameter:bayesian} shows the results of the Bayesian hyperparameter search.
Based on these results,
the initial hyperparameters are set to the values shown in \autoref{tab:hyperparameters:initial}.


\begin{figure}
  \centering
  \includegraphics[scale=1]{content/plots/hyperparam/combined_pcplot_full.pdf}
  \caption{
    Parallel coordinates plot of the initial Bayesian hyperparameter search.
    For clarity, only the mean of each 10-fold cross-validation run is shown.
  }
  \label{fig:hyperparameter:bayesian}
\end{figure}

\begin{table}
    \centering
    \caption{
      Optimal hyperparameters as determined by a Bayesian optimization search.
    }
    \label{tab:hyperparameters:initial}
    \begin{tabular}{l r}
        \toprule
        hyperparameter & {value} \\
        \midrule
        %TODO: Placeholder values
        batch size & \num{512} \\
        convergence threshold $\epsilon$ & \num{E-4} \\
        $J$ & \num{100} \\ % TODO: name?
        learning rate & \num{0.0009} \\
        \bottomrule
    \end{tabular}
\end{table}


\FloatBarrier
\subsection{Batch size}
The \emph{batch size} determines the number of events used for each training step.
While larger batch sizes increase the speed of training
on optimized hardware,
the performance of the model can be negatively affected \cite{batchsize_kandel}.
% NOTE: not-so-relevant citation

The boxplot in \autoref{fig:hyperparameter:batch_size} shows the results of a grid search for the batch size.
% As expected, the performance is worse for larger batch sizes.
A batch size of \num{4096} is chosen as the optimal value,
not only because it has the smallest Wasserstein distance,
but also because it allows for a faster training time.

\begin{figure}
  \centering
  \includegraphics[scale=1]{content/plots/hyperparam/batch_size_vs_wd_boxplot_lessheight.pdf}
  \caption{Boxplot of the performance of the model for different batch sizes.
    The performance is measured using the \emph{Wasserstein distance}.
    The box shows the 25th to 75th percentile,
    the center line denotes the median,
    % TODO: Imprecise. The whiskers extend from the box by 1.5x the inter-quartile range (IQR)
    and the whiskers show the minimum and maximum,
      apart from outliers,
        which are shown as dots.
  }
  \label{fig:hyperparameter:batch_size}
\end{figure}


\subsection{Convergence threshold}
\todo{Redundant explanation. See \autoref{sec:dsea:dsea:stepsize}.}
When using adaptive step sizes,
instead of setting a fixed number of DSEA iterations,
a minimum $\chi^2$ distance between iterations $\epsilon$
can be specified.
When the $\chi^2$ distance becomes smaller than $\epsilon$,
convergence is assumed and the training is stopped.

The boxplot in \autoref{fig:hyperparameter:epsilon} shows the results of a grid search for the convergence threshold.
For small values of $\epsilon$
  (\numrange{E-10}{E-4}),
the performance is roughly the same.

For larger values, better performance is achieved. % TODO: Up to a point. See the plot.
This is because a high convergence threshold can be thought of as
  a form of \emph{implicit regularization} \citationneeded{},
    analogous to early stopping,
  contrary to explicit regularization,
    as described in \autoref{sec:dsea:deconvolution_problem:regularization}.


\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{content/plots/hyperparam/epsilon_vs_wd_boxplot_lessheight.pdf}
  \caption{TODO.}
  \label{fig:hyperparameter:epsilon}
\end{figure}


\subsection{Adaptive step size: Number of clusters}
% TODO [WIP]: Plot and explain J instead of J-factor
The adaptive step size function,
  which has been explained in \autoref{sec:dsea:dsea:stepsize},
internally relies on clustering the data into $J$ clusters.
The number of clusters $J$ is therefore a hyperparameter of the algorithm.
%
Large values of $J$ have previously been shown to lead to slightly better results \cite{dsea_mirko}.
\autoref{fig:hyperparameter:J} confirms this,
although the median does not decrease monotonically with $J$.
The optimal value is chosen to be \num{100},
which has a slightly better performance than \num{500}.
\todo{The plots still show the $J$-factor ($J / \text{bins}$).}

\begin{figure}
  \centering
  \includegraphics[scale=1]{content/plots/hyperparam/J_factor_vs_wd_boxplot_lessheight.pdf}
  \caption{Boxplot of the Wasserstein distance for different $J$-factors.}
  \label{fig:hyperparameter:J}
\end{figure}


\subsection{Number of epochs}
While a higher number of epochs typically increases the model's performance on the training data,
it also increases the risk of overfitting.

\autoref{fig:hyperparameter:epochs} visualizes a hyperparameter search for the number of epochs per \dsea{} iteration.
The results show that the model's performance on the training data is …

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{content/plots/hyperparam/num_epochs_vs_wd_boxplot_lessheight.pdf}
  \caption{Boxplot of the Wasserstein distance for different epoch counts. TODO: Preliminary data.}
  \label{fig:hyperparameter:epochs}
\end{figure}

