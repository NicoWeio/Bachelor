\section{Dortmund Spectrum Estimation Algorithm} \label{sec:dsea:dsea}
The \acf{DSEAPLUS} \cite{dsea_unification}
is an iterative method for solving the previously stated deconvolution problem.
It improves upon its predecessor \dseanonplus{} \cite{dsea_tim} by
  correcting the re-weighting of training examples and
  giving more control over the speed of convergence.
%
A formal definition of the \dseaplus{} algorithm is given in \autoref{sec:alg:dseaplus}.


\subsection{Deconvolution as a Classification Task} % identical to Jan's paper
\dsea{} makes use of classifiers to solve the deconvolution problem.
This requires the deconvolution problem to be
  discretized (see \autoref{sec:dsea:deconvolution_problem:discretization})
  and reformulated as a multinomial classification task.

Any classifier that outputs probabilities for each class
  can be used with \dsea{}.
This is an advantage,
  as the choice of a classifier can be tailored to the specific problem at hand.
% ORIG: The novel algorithmic framework Dsea is unique among these algorithms,
% because it translates the deconvolution problem into a multinomial classification task,
% thus opening deconvolution to the field of machine learning and the rapid advances being made in that field.
Contrary to other algorithms like
  % ↓ NOTE: “TRUEE is a conversion of RUN to C++, which works within the powerful ROOT framework.”
  \ac{TRUEE} / \ac{RUN} \cite{milke2013} or
  \ac{IBU} \cite{dagostini1995, dagostini2010},
no restrictions on the input data are imposed.

Furthermore,
\dsea{} transparently provides the contributions of individual observations to the deconvolved spectrum.
This not only gives deeper insight into the performance of the algorithm,
but also allows for time-dependent deconvolution \cite{dsea_mirko}. % NOTE: … for example


\clearpage % guidance only
\subsection{Procedure}

% The \dsea{} algorithm is an iterative procedure.
% $N$ is the number of bins or classes in the spectrum.
% % $\hat \mathbf{f}_j^{(0)}$ is the initial guess for the $j$-th bin of the deconvolved spectrum.
% $\hat \mathbf{f}_j^{(k)}$ is the $j$-th bin of the deconvolved spectrum after $k$ iterations.


\subsubsection{Initialization}
Since no prior knowledge about the true spectrum is available,
  the initial spectrum is chosen to be uniform.
Given $I$ bins,
  each bin is initialized to
\begin{equation}
  % NOTE: Mirko made f bold, but not p. Jan and Samuel didn't make anything bold.
  % I'll follow Mirko's lead, for no particular reason.
  \hat \mathbf{f}_i^{(0)} = \frac{1}{I} \quad \forall i \, .
\end{equation}

The initial weights are then determined as in \autoref{eqn:dsea:weighting}.


\subsubsection{Iteration}
First,
the classifier is trained
  on the training data
to predict the class of one sample at a time,
where each sample is weighted according to its true class.
% NOTE: (break removed so this section fits on one page)
Second,
the classifier is used to predict the class of each sample
in the observed~(/test) data.
% NOTE: (break removed so this section fits on one page)
Third,
the predicted classes are used to
  % re-weight the training samples
  get an updated estimate of the spectrum
  and,
    consequently,
  updated weights for the next iteration.
%
The new spectrum is determined by the sum of the confidences of all events.
% NOTE: i (DSEA) ≙ q (CORN)
For each energy bin with index~$i$,
this can be written as
\begin{equation}
  \hat \mathbf{f}_i = \frac{1}{N} \sum_{n=1}^N \hat c_{i,n} \, ,
\end{equation}
where $N$ is the number of events in the observation data set,
$k$ is the current iteration number, % Oxford comma
and $\hat c_{i,n}$ is the confidence
  that event $n$ belongs to class $i$.
The factor $\sfrac{1}{N}$ is introduced to normalize the spectrum
to a true probability density distribution.
%
% █ The resulting spectrum is the preliminary deconvolution result.
The weights of the training samples are then updated according to the new spectrum~$\hat \mathbf{f}_i^{(k)}$. % …preliminary deconvolution result.
In \dseaplus{}, the reconstructed spectrum is divided by the training spectrum % NOTE: → fixweighting
  in order to mitigate the impact of the training spectrum on the deconvolution result.
A more detailed reasoning is given in \cite{dsea_mirko}.
The updated weights are given by
\begin{equation}
  \label{eqn:dsea:weighting}
  w_i^{(k+1)} = \frac{\hat \mathbf{f}_i^{(k)}}{\mathbf{f}_i^\text{train}} \, ,
\end{equation}
where $w_i^{(k+1)}$ is the weight applied to training samples with true~bin~$i$ in iteration~$k+1$, % (Oxford comma)
% $\hat \mathbf{f}_i^{(k)}$ is
%   the value of the $i$-th bin in
%   the current deconvolution result, % Oxford comma
and $\mathbf{f}_i^\text{train}$ is
  the value of the $i$-th bin in
  the training spectrum.


The iterative procedure is repeated
  with the updated weights
until
  convergence
    (see \autoref{sec:dsea:dsea:step_size})
    % (determined by the $\chi^2$ distance between the current and previous deconvolution result)
  or,
    in case of fixed step sizes,
  a maximum number of iterations % $K$
is reached.
%
%
% \subsubsection{Result}
The final deconvolution result is the spectrum obtained in the last iteration.


\clearpage % guidance only
\subsection{Step Size Functions} \label{sec:dsea:dsea:step_size}
\dseaplus{} introduces the concept of a step size $\alpha$,
which allows the user to control the speed of convergence,
which in turn has a significant impact on the quality of the result.

A \emph{step} $p_i^{(k)}$ is the difference between the current and previous deconvolution result:
\begin{equation}
  p_i^{(k)} = \hat \mathbf{f}_i^{(k)} - \hat \mathbf{f}_i^{(k-1)} \, .
\end{equation}
Instead of updating the spectrum with the current deconvolution result $\mathbf{f}_i^{(k)}$ directly,
the step is multiplied with the step size
and added to the previous deconvolution result:
\begin{equation}
  \hat \mathbf{f}_i^{(k)+} = \hat \mathbf{f}_i^{(k-1)} + \alpha \cdot p_i^{(k)} \, .
\end{equation}
This improved estimate $\hat \mathbf{f}_i^{(k)+}$ is then considered instead of $\hat \mathbf{f}_i^{(k)}$.

While the original \dseanonplus{} algorithm uses a fixed step size of $\alpha = 1$,
\dseaplus{} allows arbitrary constants $\alpha > 0$
or functions of the iteration number $k$.
Commonly used step size functions include
multiplicative decay
  $\alpha^{(k)} = k^{\eta - 1}$
and exponential decay
  $\alpha^{(k)} = \eta^{(k - 1)}$,
each with a \emph{decay rate} $0 < \eta < 1$.
%
These decaying step sizes ensure that the algorithm converges,
decreasing the importance of the maximum number of iterations $K$,
while enabling the use of a stopping criterion:
  When the $\chi^2$~distance becomes smaller than $\epsilon$,
  convergence is assumed and the training is stopped.
  % NOTE: This had already been suggested in the original DSEA paper (→ dsea_tim).
%
In this work,
the probabilistic symmetric $\chi^2$~distance \cite{chisquare, dsea_mirko}
\begin{equation}
  \newcommand{\myf}{\mathbf{f}_i}
  \newcommand{\myfhat}{\hat\myf}
  % \chi^2 =
  \chi^2_\text{Sym}(\hat\mathbf{f}, \mathbf{f}) =
  2 \cdot \sum_{i=1}^I \frac{(\myfhat - \myf)^2}{\myfhat + \myf}
\end{equation}
is used.

\phantomsection \label{sec:dsea:dsea:step_size:adaptive}
The utilization of \emph{adaptive step sizes} \cite{dsea_mirko}
can further improve the convergence of the algorithm
  by choosing an optimal step size for each iteration. % COULDDO: language
This is achieved by searching
  along the direction of the step $p_i^{(k)}$ % aka. search direction
  for the step size $\alpha \geq 0$
    which minimizes the \ac{RUN} \cite{milke2013} loss function.
    % COULDDO: Add more details about the RUN loss function.
In the process,
the method discretizes the training data
  using a decision tree,
  thus adding a hyperparameter $J$
    that controls the number of its leaves.
